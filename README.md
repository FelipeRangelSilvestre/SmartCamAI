# SmartCam AI

## ğŸ¯ O que nossa aplicaÃ§Ã£o faz

O **SmartCam AI** Ã© uma aplicaÃ§Ã£o de visÃ£o computacional que reconhece gestos das mÃ£os em tempo real usando a cÃ¢mera do dispositivo mÃ³vel, mas com um diferencial revolucionÃ¡rio: **consome 10 vezes menos bateria** que as soluÃ§Ãµes tradicionais.

### Funcionalidade Principal
A aplicaÃ§Ã£o fica "sempre ligada" no background do seu celular, monitorando continuamente atravÃ©s da cÃ¢mera para detectar gestos especÃ­ficos das mÃ£os. Quando vocÃª faz um gesto reconhecido (como acenar, apontar, ou fazer um sinal de "ok"), a aplicaÃ§Ã£o pode:

- Ativar funcionalidades do telefone sem precisar tocar na tela
- Controlar aplicativos atravÃ©s de comandos gestuais  
- Responder a comandos visuais mesmo quando o celular estÃ¡ na mesa

### O Diferencial: EficiÃªncia EnergÃ©tica
**Problema atual**: AplicaÃ§Ãµes que usam cÃ¢mera constantemente drenam a bateria muito rÃ¡pido, tornando impraticÃ¡vel manter a funcionalidade sempre ativa.

**Nossa soluÃ§Ã£o**: Utilizamos uma tecnologia inspirada no funcionamento do cÃ©rebro humano (Redes Neurais Spiking) que processa informaÃ§Ãµes visuais de forma muito mais eficiente, permitindo que a aplicaÃ§Ã£o funcione o dia todo sem impactar significativamente a bateria.

## ğŸ—ºï¸ Roadmap do Projeto

O projeto estÃ¡ estruturado em trÃªs etapas principais:

### Etapa 1: Linha de Base (Baseline) âš¡
- **Objetivo**: Estabelecer uma referÃªncia de desempenho
- **Tecnologia**: CNN tradicional (MobileNetV3)
- **Status**: ğŸ”„ Em desenvolvimento - processamento de dados

### Etapa 2: TransiÃ§Ã£o NeuromÃ³rfica ğŸ§ 
- **Objetivo**: Converter CNN para SNN otimizada
- **Foco**: MÃ¡xima eficiÃªncia energÃ©tica
- **Status**: â³ Aguardando conclusÃ£o da Etapa 1

### Etapa 3: DemonstraÃ§Ã£o e AnÃ¡lise ğŸ“Š
- **Objetivo**: SimulaÃ§Ã£o em hardware neuromÃ³rfico
- **Entrega**: ComparaÃ§Ã£o completa com baseline
- **Status**: â³ Planejado

## ğŸ”§ ConfiguraÃ§Ã£o do Ambiente

### PrÃ©-requisitos
- Python 3.x
- pip (gerenciador de pacotes Python)

### ğŸ”§ ConfiguraÃ§Ã£o de DependÃªncias
```bash
py -m pip install opencv-python pandas tqdm
```

**DependÃªncias utilizadas:**
- `opencv-python` - Processamento de imagem e vÃ­deo
- `pandas` - ManipulaÃ§Ã£o dos arquivos CSV de legendas  
- `tqdm` - Barras de progresso durante processamento

### Dataset
Este projeto utiliza o dataset **20BN-Jester V1** para reconhecimento de gestos:
- [Download dos vÃ­deos](https://20bn.com/datasets/jester) - Frames em formato JPG organizados por pasta
- [Download das legendas](https://20bn.com/datasets/jester) - Arquivos CSV separados

**Arquivos necessÃ¡rios:**
```
projeto/
â”œâ”€â”€ 20bn-jester-v1/              # Pasta com subpastas numeradas (cada uma contÃ©m frames JPG)
â”œâ”€â”€ jester-v1-labels.csv         # Lista completa de todos os gestos
â”œâ”€â”€ jester-v1-train.csv          # Mapeamento ID â†’ gesto (treino)
â”œâ”€â”€ jester-v1-validation.csv     # Mapeamento ID â†’ gesto (validaÃ§Ã£o)
â””â”€â”€ scripts/
    â”œâ”€â”€ process_jester.py        # Processamento principal
    â””â”€â”€ inspect_labels.py        # DiagnÃ³stico de gestos
```

## ğŸ“ Estrutura do Projeto

```
smartcam-ai/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ process_jester.py     # Processamento principal do dataset
â”‚   â”œâ”€â”€ inspect_labels.py     # InspeÃ§Ã£o de rÃ³tulos
â”‚   â””â”€â”€ train_model.py        # [Em desenvolvimento] Treinamento do modelo
â”œâ”€â”€ data/
â”‚   â””â”€â”€ processed/           # Dados processados
â”œâ”€â”€ models/
â”‚   â””â”€â”€ baseline/           # Modelos da linha de base
â””â”€â”€ docs/
    â””â”€â”€ progress_report.html # RelatÃ³rio de progresso
```

## ğŸš€ Como Usar

### 1. IdentificaÃ§Ã£o dos Gestos DisponÃ­veis
Primeiro, execute o script de diagnÃ³stico para ver todos os gestos disponÃ­veis no dataset:
```bash
py inspect_labels.py
```
Este script lÃª o arquivo `jester-v1-labels.csv` e lista todos os gestos Ãºnicos disponÃ­veis.

### 2. Processamento de Dados
ApÃ³s identificar os gestos desejados, atualize a lista `TARGET_GESTURES` no arquivo `process_jester.py` e execute:
```bash
py process_jester.py
```

**Gestos atualmente configurados:**
- `Thumb up` - Sinal de "positivo" com o polegar
- `Stop Sign` - Sinal de "pare" com a mÃ£o aberta

### 3. Treinamento (Em desenvolvimento)
```bash
py train_model.py
```

## ğŸ› Problemas Conhecidos e SoluÃ§Ãµes

### Problema: 'python' nÃ£o Ã© reconhecido
**SoluÃ§Ã£o**: Use `py` em vez de `python` no Windows, ou configure a variÃ¡vel PATH.

### Problema: ModuleNotFoundError 
**SoluÃ§Ã£o**: 
```bash
py -m pip install [nome_do_modulo]
```

### Problema: Arquivo 'jester-v1-labels.csv' nÃ£o encontrado
**SoluÃ§Ã£o**: Baixe todos os arquivos CSV de legendas separadamente do site do dataset 20BN-Jester. SÃ£o 3 arquivos:
- `jester-v1-labels.csv` (lista de gestos)
- `jester-v1-train.csv` (mapeamento treino)  
- `jester-v1-validation.csv` (mapeamento validaÃ§Ã£o)

### Problema: Gestos nÃ£o encontrados no processamento
**SoluÃ§Ã£o**: Execute `py inspect_labels.py` para ver os nomes exatos dos gestos e atualize a lista `TARGET_GESTURES` no arquivo `process_jester.py`.

## ğŸ“Š Status Atual

### âœ… ConcluÃ­do
- [x] ConfiguraÃ§Ã£o do ambiente de desenvolvimento
- [x] ResoluÃ§Ã£o de problemas de dependÃªncias  
- [x] Script de processamento de dados (`process_jester.py`)
- [x] Script de diagnÃ³stico de gestos (`inspect_labels.py`)
- [x] IdentificaÃ§Ã£o dos gestos corretos no dataset

### ğŸ”„ Em Progresso
- [ ] Processamento final dos frames de vÃ­deo para os gestos selecionados
- [ ] ValidaÃ§Ã£o da qualidade dos dados processados

### â³ PrÃ³ximos Passos
- [ ] Desenvolvimento do script de treinamento (`train_model.py`)
- [ ] ImplementaÃ§Ã£o da CNN baseline (MobileNetV3)
- [ ] MÃ©tricas de desempenho e consumo energÃ©tico
- [ ] ConversÃ£o para SNN (Etapa 2)

## ğŸ¤ ContribuiÃ§Ã£o

Este Ã© um projeto de pesquisa em desenvolvimento. Para contribuir:

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob licenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.

## ğŸ“§ Contato

Para dÃºvidas ou colaboraÃ§Ãµes, entre em contato atravÃ©s dos issues do GitHub.

---

**Nota**: Este projeto estÃ¡ em desenvolvimento ativo. A documentaÃ§Ã£o serÃ¡ atualizada conforme o progresso das etapas.

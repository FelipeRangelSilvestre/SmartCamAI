# SmartCam AI

## ğŸ¯ O que nossa aplicaÃ§Ã£o faz

O **SmartCam AI** Ã© uma aplicaÃ§Ã£o de visÃ£o computacional que reconhece gestos das mÃ£os em tempo real usando a cÃ¢mera do dispositivo mÃ³vel, mas com um diferencial revolucionÃ¡rio: **consome 10 vezes menos bateria** que as soluÃ§Ãµes tradicionais.

### Funcionalidade Principal
A aplicaÃ§Ã£o fica "sempre ligada" no background do seu celular, monitorando continuamente atravÃ©s da cÃ¢mera para detectar gestos especÃ­ficos das mÃ£os. Quando vocÃª faz um gesto reconhecido (como acenar, apontar, ou fazer um sinal de "ok"), a aplicaÃ§Ã£o pode:

- Ativar funcionalidades do telefone sem precisar tocar na tela
- Controlar aplicativos atravÃ©s de comandos gestuais  
- Responder a comandos visuais mesmo quando o celular estÃ¡ na mesa

### O Diferencial: EficiÃªncia EnergÃ©tica
**Problema atual**: AplicaÃ§Ãµes que usam cÃ¢mera constantemente drenam a bateria muito rÃ¡pido, tornando impraticÃ¡vel manter a funcionalidade sempre ativa.

**Nossa soluÃ§Ã£o**: Utilizamos uma tecnologia inspirada no funcionamento do cÃ©rebro humano (Redes Neurais Spiking) que processa informaÃ§Ãµes visuais de forma muito mais eficiente, permitindo que a aplicaÃ§Ã£o funcione o dia todo sem impactar significativamente a bateria.

## ğŸ—ºï¸ Roadmap do Projeto

O projeto estÃ¡ estruturado em trÃªs etapas principais:

### Etapa 1: Linha de Base (Baseline) âš¡
- **Objetivo**: Estabelecer uma referÃªncia de desempenho
- **Tecnologia**: CNN tradicional (MobileNetV3)
- **Status**: ğŸ”„ Em desenvolvimento - processamento de dados

### Etapa 2: TransiÃ§Ã£o NeuromÃ³rfica ğŸ§ 
- **Objetivo**: Converter CNN para SNN otimizada
- **Foco**: MÃ¡xima eficiÃªncia energÃ©tica
- **Status**: â³ Aguardando conclusÃ£o da Etapa 1

### Etapa 3: DemonstraÃ§Ã£o e AnÃ¡lise ğŸ“Š
- **Objetivo**: SimulaÃ§Ã£o em hardware neuromÃ³rfico
- **Entrega**: ComparaÃ§Ã£o completa com baseline
- **Status**: â³ Planejado

## ğŸ”§ ConfiguraÃ§Ã£o do Ambiente

### PrÃ©-requisitos
- Python 3.x
- pip (gerenciador de pacotes Python)

### InstalaÃ§Ã£o de DependÃªncias
```bash
py -m pip install opencv-python pandas tqdm
```

### Dataset
Este projeto utiliza o dataset **20BN-Jester V1** para reconhecimento de gestos:
- [Download dos vÃ­deos](https://20bn.com/datasets/jester)
- [Download das legendas](https://20bn.com/datasets/jester) (arquivos .csv separados)

Estrutura de arquivos esperada:
```
projeto/
â”œâ”€â”€ 20bn-jester-v1/          # VÃ­deos do dataset
â”œâ”€â”€ jester-v1-train.csv      # Legendas de treino
â”œâ”€â”€ jester-v1-validation.csv # Legendas de validaÃ§Ã£o
â””â”€â”€ scripts/
    â”œâ”€â”€ process_jester.py    # Processamento de dados
    â””â”€â”€ inspect_labels.py    # DiagnÃ³stico de gestos
```

## ğŸ“ Estrutura do Projeto

```
smartcam-ai/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ process_jester.py     # Processamento principal do dataset
â”‚   â”œâ”€â”€ inspect_labels.py     # InspeÃ§Ã£o de rÃ³tulos
â”‚   â””â”€â”€ train_model.py        # [Em desenvolvimento] Treinamento do modelo
â”œâ”€â”€ data/
â”‚   â””â”€â”€ processed/           # Dados processados
â”œâ”€â”€ models/
â”‚   â””â”€â”€ baseline/           # Modelos da linha de base
â””â”€â”€ docs/
    â””â”€â”€ progress_report.html # RelatÃ³rio de progresso
```

## ğŸš€ Como Usar

### 1. Processamento de Dados
Primeiro, execute o script de diagnÃ³stico para identificar os gestos disponÃ­veis:
```bash
py inspect_labels.py
```

Em seguida, processe o dataset:
```bash
py process_jester.py
```

### 2. Treinamento (Em desenvolvimento)
```bash
py train_model.py
```

## ğŸ› Problemas Conhecidos e SoluÃ§Ãµes

### Problema: 'python' nÃ£o Ã© reconhecido
**SoluÃ§Ã£o**: Use `py` em vez de `python` no Windows, ou configure a variÃ¡vel PATH.

### Problema: ModuleNotFoundError
**SoluÃ§Ã£o**: 
```bash
py -m pip install [nome_do_modulo]
```

### Problema: Arquivos .csv nÃ£o encontrados
**SoluÃ§Ã£o**: Baixe os arquivos de legenda separadamente do site do dataset 20BN-Jester.

## ğŸ“Š Status Atual

### âœ… ConcluÃ­do
- [x] ConfiguraÃ§Ã£o do ambiente de desenvolvimento
- [x] ResoluÃ§Ã£o de problemas de dependÃªncias
- [x] Script de processamento de dados funcional
- [x] Script de diagnÃ³stico de rÃ³tulos

### ğŸ”„ Em Progresso
- [ ] IdentificaÃ§Ã£o dos gestos corretos no dataset
- [ ] Processamento final dos frames de vÃ­deo
- [ ] AtualizaÃ§Ã£o da lista `TARGET_GESTURES`

### â³ PrÃ³ximos Passos
- [ ] Desenvolvimento do script de treinamento
- [ ] ImplementaÃ§Ã£o da CNN baseline (MobileNetV3)
- [ ] MÃ©tricas de desempenho e consumo energÃ©tico
- [ ] ConversÃ£o para SNN (Etapa 2)

## ğŸ¤ ContribuiÃ§Ã£o

Este Ã© um projeto de pesquisa em desenvolvimento. Para contribuir:

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob licenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.

## ğŸ“§ Contato

Para dÃºvidas ou colaboraÃ§Ãµes, entre em contato atravÃ©s dos issues do GitHub.

---

**Nota**: Este projeto estÃ¡ em desenvolvimento ativo. A documentaÃ§Ã£o serÃ¡ atualizada conforme o progresso das etapas.
